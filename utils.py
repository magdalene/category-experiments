import json
import os
import re
import string

from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer

stemmer = PorterStemmer()

SKIP_TITLE_REGEX = re.compile(r'.*title=\"[Wikinews\:|MediaWiki\:|File\:|Portal\:|Comments\:|Category\:|Help\:].*')

def tokenize(text):
    """Tokenize and stem a single text.

    Args:
        text: string, raw text to tokenize
    Returns: list of strings, tokens of the text
    """
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if not all([c in string.punctuation for c in w])]
    return [stemmer.stem(w) for w in tokens]


def check_doc(doc):
    """Check whether this is a real news story document.

    There are several documents which are not "real" documents, instead they contain wikinews/wikimedia
    info (about comment policy, blocking, files, etc.). They have title prefixes, which we can match
    to filter them out.

    Args:
        doc: dict representation of the doc, generated by Wikiextractor.py with --json option
    Returns: boolean, whether this is a valid text or not
    """
    if SKIP_TITLE_REGEX.match(doc['title']):
        return False
    return True


def load_data(data_dir):
    """Load a list of texts from a data directory.

    Args:
        data_dir: string, directory to load data from
    Returns: list of dicts, raw information dicts as generated by Wikiextractor.py with --json option.
    """
    docs = []
    for filename in os.listdir(data_dir):
        with open(os.path.join(data_dir, filename), 'rt', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                if check_doc(data):
                    docs.append(data)
    return docs


def get_category_info(categories_dir):
    """Load category info from human-labeled files."""
    high_level_cats = set()
    low_level_cats = set()
    for filename in os.listdir(categories_dir):
        with open(os.path.join(categories_dir, filename), 'rt', encoding='utf-8') as f:
            catname = filename.replace('_', ' ').replace('.csv', '')
            high_level_cats.add(catname)
            for line in f:
                line_split = line.strip().split(',')
                if line_split[-1].startswith('yes'):
                    low_level_cats.add(line_split[0])
    return high_level_cats, low_level_cats