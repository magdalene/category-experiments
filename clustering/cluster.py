"""Script to create "topic" clusters from wikinews documents."""
import os
import pickle
import re
import string

import click
from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

stemmer = PorterStemmer()


SKIP_TITLE_REGEX = re.compile(r'.*title=\"[Wikinews\:|MediaWiki\:|File\:|Portal\:|Comments\:|Category\:|Help\:].*')


def tokenize(text):
    """Tokenize and stem a single text.

    Args:
        text: string, raw text to tokenize
    Returns: list of strings, tokens of the text
    """
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if not all([c in string.punctuation for c in w])]
    return [stemmer.stem(w) for w in tokens]


def next_doc(f):
    """Get the next doc from a wikiextractor script xml-like output file stream.

    Args:
        f: file pointer (or any iterable of lines)
    Returns: list of strings, the lines of the next doc
    """
    lines = []
    while True:
        try:
            line = next(f)
        except StopIteration:
            return None
        lines.append(line)
        # This looks ugly, but wikiextractor script uses xml-like markers, but output isn't actually
        # valid xml -- just <doc...> and </doc...> tags with plaintext (not escaped) in between;
        # it's not expected to be parsed as xml
        if line.strip().startswith('</doc'):
            return lines


def check_doc(doc_lines):
    """Check whether this is a real news story document.

    There are several documents which are not "real" documents, instead they contain wikinews/wikimedia
    info (about comment policy, blocking, files, etc.). They have title prefixes, which we can match
    to filter them out.

    Args:
        doc_lines: list of string, lines from a doc
    Returns: boolean, whether this is a valid text or not
    """
    if not len(doc_lines):
        return False
    elif SKIP_TITLE_REGEX.match(doc_lines[0]):
        return False
    return True


def load_data(data_dir):
    """Load a list of texts from a data directory.

    Args:
        data_dir: string, directory to load data from
    Returns: list of strings, raw texts.
    """
    data = []
    for filename in os.listdir(data_dir):
        with open(os.path.join(data_dir, filename), 'rt', encoding='utf-8') as f:
            doc_lines = next_doc(f)
            while doc_lines:
                if check_doc(doc_lines):
                    data.append('\n'.join(doc_lines[1:-1]))
                doc_lines = next_doc(f)
    return data


def cluster(raw_data, n_clusters):
    """Do kmeans clustering, and return the computed results.

    Args:
        raw_data: list of strings, texts to cluster
        n_clusters: int, number of clusters
    Returns: kmeans model, vectorizer, raw text clusters
    """
    vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')
    data = vectorizer.fit_transform(raw_data)
    kmeans = KMeans(n_clusters=n_clusters).fit(data)
    predictions = kmeans.predict(data)
    clusters = {}
    for prediction, raw_doc in zip(predictions, raw_data):
        clusters.setdefault(prediction, [])
        clusters[prediction].append(raw_doc)
    return kmeans, vectorizer, clusters


@click.command()
@click.option('--input_dir', '-i', help='Input directory, of files generated by Wikiextractor.py')
@click.option('--output_dir', '-o', help='Output dir, for the pickled cluster model and the raw doc clusters')
@click.option('--n_clusters', '-n', default=500, help='Number of clusters')
def main(input_dir, output_dir, n_clusters):
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)
    clusters_dir = os.path.join(output_dir, 'clusters')
    if not os.path.exists(clusters_dir):
        os.mkdir(clusters_dir)

    data = load_data(input_dir)
    model, vect, doc_clusters = cluster(data, n_clusters)
    with open(os.path.join(output_dir, 'model.pkl'), 'wb') as f:
        pickle.dump(model, f)
    with open(os.path.join(output_dir, 'vectorizer.pkl'), 'wb') as f:
        pickle.dump(vect, f)
    for i, doc_cluster in doc_clusters.items():
        with open(os.path.join(clusters_dir, 'cluster_{}'.format(i)), 'wt', encoding='utf-8') as f:
            cluster_docs_string = '<doc>\n' + '\n</doc>\n<doc>\n'.join(doc_cluster) + '\n</doc>'
            f.write(cluster_docs_string)


if __name__ == '__main__':
    main()
