"""Script to create "topic" clusters from wikinews documents."""
import json
import os
import pickle
import re
import string

import click
from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

stemmer = PorterStemmer()

SKIP_TITLE_REGEX = re.compile(r'.*title=\"[Wikinews\:|MediaWiki\:|File\:|Portal\:|Comments\:|Category\:|Help\:].*')


def tokenize(text):
    """Tokenize and stem a single text.

    Args:
        text: string, raw text to tokenize
    Returns: list of strings, tokens of the text
    """
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if not all([c in string.punctuation for c in w])]
    return [stemmer.stem(w) for w in tokens]


def check_doc(doc):
    """Check whether this is a real news story document.

    There are several documents which are not "real" documents, instead they contain wikinews/wikimedia
    info (about comment policy, blocking, files, etc.). They have title prefixes, which we can match
    to filter them out.

    Args:
        doc: dict representation of the doc, generated by Wikiextractor.py with --json option
    Returns: boolean, whether this is a valid text or not
    """
    if SKIP_TITLE_REGEX.match(doc['title']):
        return False
    return True


def load_data(data_dir):
    """Load a list of texts from a data directory.

    Args:
        data_dir: string, directory to load data from
    Returns: list of dicts, raw information dicts as generated by Wikiextractor.py with --json option.
    """
    docs = []
    for filename in os.listdir(data_dir):
        with open(os.path.join(data_dir, filename), 'rt', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                if check_doc(data):
                    docs.append(data)
    return docs


def cluster(text_infos, n_clusters):
    """Do kmeans clustering, and return the computed results.

    Args:
        text_infos: list of dicts, info about texts to cluster
        n_clusters: int, number of clusters
    Returns: kmeans model, vectorizer, raw text clusters
    """
    vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')
    data = vectorizer.fit_transform([text_info['text'] for text_info in text_infos])
    kmeans = KMeans(n_clusters=n_clusters).fit(data)
    predictions = kmeans.predict(data)
    clusters = {}
    for prediction, text_info in zip(predictions, text_infos):
        clusters.setdefault(prediction, [])
        clusters[prediction].append(text_info)
    return kmeans, vectorizer, clusters


@click.command()
@click.option('--input_dir', '-i', help='Input directory, of files generated by Wikiextractor.py')
@click.option('--output_dir', '-o', help='Output dir, for the pickled cluster model and the raw doc clusters')
@click.option('--n_clusters', '-n', default=500, help='Number of clusters')
def main(input_dir, output_dir, n_clusters):
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)
    clusters_dir = os.path.join(output_dir, 'clusters')
    if not os.path.exists(clusters_dir):
        os.mkdir(clusters_dir)

    data = load_data(input_dir)
    model, vect, doc_clusters = cluster(data, n_clusters)
    with open(os.path.join(output_dir, 'model.pkl'), 'wb') as f:
        pickle.dump(model, f)
    with open(os.path.join(output_dir, 'vectorizer.pkl'), 'wb') as f:
        pickle.dump(vect, f)
    for i, doc_cluster in doc_clusters.items():
        with open(os.path.join(clusters_dir, 'cluster_{}'.format(i)), 'wt', encoding='utf-8') as f:
            cluster_docs_string = '\n'.join([json.dumps(doc) for doc in doc_cluster])
            f.write(cluster_docs_string)


if __name__ == '__main__':
    main()
